

# 误差反向传播法

[TOC]

上一章中，我们介绍了神经网络的学习，并通过数值微分计算了神经网络的权重参数的梯度（严格来说，是损失函数关于权重参数的梯度）。数值微分虽然简单，也容易实现，但**缺点**是计算上比较费时间。本章我们将学习一个能够高效计算权重参数的梯度的方法——误差反向传播法。

要正确理解误差反向传播法，我个人认为有两种方法：一种是**基于数学式**；另一种是**基于计算图**（computational graph）。前者是比较常见的方法，机器学习相关的图书中多数都是以数学式为中心展开论述的。因为这种方法严密且简洁，所以确实非常合理，但如果一上来就围绕数学式进行探讨，会忽略一些根本的东西，止步于式子的罗列。因此，本章希望大家通过计算图，直观地理解误差反向传播法。然后，再结合实际的代码加深理解，相信大家一定会有种“原来如此！”的感觉。

## 计算图
计算图将计算过程用图形表示出来。这里说的图形是数据结构图，通过多个节点和边表示（连接节点的直线称为“边”）。为了让大家熟悉计算图，本节先用计算图解一些简单的问题。从这些简单的问题开始，逐步深入，最终抵达误差反向传播法。

### 用计算图求解

**问题 1**：太郎在超市买了 2 个 100 日元一个的苹果，消费税是 10%，请计算支付金额。

![059](https://ws3.sinaimg.cn/large/006tKfTcly1g076nlaz88j30rs08idh5.jpg)

**图 5-2　基于计算图求解的问题 1 的答案：“苹果的个数”和“消费税”作为变量标在○外面**

用计算图解题的情况下，需要按如下流程进行：
- 构建计算图
- 在计算图上，从左向右进行计算

这里的第 2 歩“从左向右进行计算”是一种正方向上的传播，简称为正向传播（forward propagation）。正向传播是从计算图出发点到结束点的传播。既然有正向传播这个名称，当然也可以考虑反向（从图上看的话，就是从右向左）的传播。实际上，这种传播称为反向传播（backward propagation）。反向传播将在接下来的导数计算中发挥重要作用。

### 局部计算
计算图可以集中精力于局部计算。无论全局的计算有多么复杂，各个步骤所要做的就是对象节点的局部计算。虽然局部计算非常简单，但是通过传递它的计算结果，可以获得全局的复杂计算的结果。

计算图将复杂的计算分割成简单的局部计算，和流水线作业一样，将局部计算的结果传递给下一个节点。在将复杂的计算分解成简单的计算这一点上与汽车的组装有相似之处。

### 为何用计算图解题
计算图到底有什么优点：
- 局部计算：无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题
- 利用计算图可以将中间的计算结果全部保存起来
- 使用计算图最大的原因是，可以通过反向传播高效计算导数

综上，计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值。

问题 1 中，我们计算了购买 2 个苹果时加上消费税最终需要支付的金额。这里，假设我们想知道苹果价格的上涨会在多大程度上影响最终的支付金额，即求“支付金额关于苹果的价格的导数”。

![062](https://ws1.sinaimg.cn/large/006tKfTcly1g076o6b09qj30rs08pgn4.jpg)

**图 5-5　基于反向传播的导数的传递**

如图 5-5 所示，反向传播使用与正方向相反的箭头（粗线）表示。反向传播传递“局部导数”，将导数的值写在箭头的下方。在这个例子中，反向传播从右向左传递导数的值（1 → 1.1 → 2.2）。从这个结果中可知，“支付金额关于苹果的价格的导数”的值是 2.2。这意味着，如果苹果的价格上涨 1 日元，最终的支付金额会增加 2.2 日元（严格地讲，如果苹果的价格增加某个微小值，则最终的支付金额将增加那个微小值的 2.2 倍）。

这里只求了关于苹果的价格的导数，不过“支付金额关于消费税的导数”“支付金额关于苹果的个数的导数”等也都可以用同样的方式算出来。并且，计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数。综上，计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值。

## 链式法则
前面介绍的计算图的正向传播将计算结果正向（从左到右）传递，其计算过程是我们日常接触的计算过程，所以感觉上可能比较自然。而反向传播将局部导数向正方向的反方向（从右到左）传递，一开始可能会让人感到困惑。传递这个局部导数的原理，是基于**链式法则**（chain rule）的。本节将介绍链式法则，并阐明它是如何对应计算图上的反向传播的。

### 计算图的反向传播
计算图的反向传播：沿着与正方向相反的方向，乘上局部导数。

这就是反向传播的计算顺序。通过这样的计算，可以高效地求出导数的值，这是反向传播的要点。那么这是如何实现的呢？我们可以从链式法则的原理进行解释。下面我们就来介绍链式法则。

### 什么是链式法则
介绍链式法则时，我们需要先从**复合函数**说起。复合函数是由多个函数构成的函数。

链式法则是关于**复合函数**的导数的性质：

```text
如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示。
```

### 链式法则和计算图
计算图：沿着与正方向相反的方向，乘上局部导数后传递

## 反向传播
上一节介绍了计算图的反向传播是基于链式法则成立的。本节将以“+”和“×”等运算为例，介绍反向传播的结构。

### 加法节点的反向传播

加法节点的反向传播将上游的值原封不动地输出到下游。

### 乘法节点的反向传播
![070](https://ws1.sinaimg.cn/large/006tKfTcly1g0776h7kmej30rs0ba0tt.jpg)

**图 5-12　乘法的反向传播：左图是正向传播，右图是反向传播**

乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游。翻转值表示一种翻转关系，如图 5-12 所示，正向传播时信号是 x 的话，反向传播时则是 y；正向传播时信号是 y 的话，反向传播时则是 x。

法的反向传播只是将上游的值传给下游，并不需要正向传播的输入信号。但是，乘法的反向传播需要正向传播时的输入信号值。因此，实现乘法节点的反向传播时，要保存正向传播的输入信号。

### 苹果的例子

再来思考一下本章最开始举的购买苹果的例子（2 个苹果和消费税）。这里要解的问题是苹果的价格、苹果的个数、消费税这 3 个变量各自如何影响最终支付的金额。这个问题相当于求“支付金额关于苹果的价格的导数”“支付金额关于苹果的个数的导数”“支付金额关于消费税的导数”。用计算图的反向传播来解的话，求解过程如图 5-14 所示。

![072](https://ws4.sinaimg.cn/large/006tKfTcly1g0777ihyp8j30rs0a4wgh.jpg)

**图 5-14　购买苹果的反向传播的例子**

## 简单层的实现

本节将用 Python 实现前面的购买苹果的例子。这里，我们把要实现的计算图的乘法节点称为“乘法层”（`MulLayer`），加法节点称为“加法层”（`AddLayer`）。

```text
下一节，我们将把构建神经网络的“层”实现为一个类。这里所说的“层”是神经网络中功能的单位。比如，负责 sigmoid 函数的 Sigmoid、负责矩阵乘积的 Affine 等，都以层为单位进行实现。因此，这里也以层为单位来实现乘法节点和加法节点。
```

### 乘法层的实现

层的实现中有两个共通的方法（接口）`forward()` 和 `backward()`。`forward()` 对应正向传播，`backward()` 对应反向传播。

现在来实现乘法层。乘法层作为 `MulLayer` 类，其实现过程如下所示

````python
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None

    def forward(self, x, y):
        self.x = x
        self.y = y
        out = x * y

        return out

    def backward(self, dout):
        dx = dout * self.y # 翻转x和y
        dy = dout * self.x

        return dx, dy
````

### 加法层的实现

```python
class AddLayer:
    def __init__(self):
        pass

    def forward(self, x, y):
        out = x + y
        return out

    def backward(self, dout):
        dx = dout * 1
        dy = dout * 1
        return dx, dy
```

## 激活函数层的实现

现在，我们将计算图的思路应用到神经网络中。这里，我们把构成神经网络的层实现为一个类。先来实现激活函数的 `ReLU` 层和 `Sigmoid` 层。

### ReLU 层
激活函数 ReLU（Rectified Linear Unit）由下式（5.7）表示。

![188](https://ws2.sinaimg.cn/large/006tKfTcly1g077miwnvrj30x50680sw.jpg)

通过式（5.7），可以求出 *y* 关于 *x* 的导数，如式（5.8）所示。

![189](https://ws2.sinaimg.cn/large/006tKfTcly1g077mllfkqj30xp06aq36.jpg)

如果正向传播时的输入 x 大于 0，则反向传播会将上游的值原封不动地传给下游。反过来，如果正向传播时的 x 小于等于 0，则反向传播中传给下游的信号将停在此处。

![076](https://ws4.sinaimg.cn/large/006tKfTcly1g077quqdbaj30rs06o3zb.jpg)

**图 5-18　ReLU 层的计算图**

```python
class Relu:
    def __init__(self):
        self.mask = None

    def forward(self, x):
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0

        return out

    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout

        return dx
```

```text
ReLU 层的作用就像电路中的开关一样。正向传播时，有电流通过的话，就将开关设为 ON；没有电流通过的话，就将开关设为 OFF。反向传播时，开关为 ON 的话，电流会直接通过；开关为 OFF 的话，则不会有电流通过。
```

### Sigmoid 层
sigmoid 函数由式（5.9）表示。

![gif](https://ws4.sinaimg.cn/large/006tKfTcly1g077r6uudtg30770150mq.gif)

用计算图表示式（5.9）的话，则如图 5-19 所示。

![077](https://ws2.sinaimg.cn/large/006tKfTcly1g077rplchoj30rs06hmxx.jpg)





![](https://note.youdao.com/yws/api/personal/file/WEBb80b06e14402edc2713c0a8786695e53?method=download&shareKey=4bb2fbbed6e9ffafe47accefb1c087be)





![](https://note.youdao.com/yws/api/personal/file/WEB060508652aac5da164d3394bc9777b6b?method=download&shareKey=21050fc18d504494022c4ee20a31df92)

![](https://note.youdao.com/yws/api/personal/file/WEB876fcd0a1642c652c8fd6052f57937f9?method=download&shareKey=91e837041cddb8e988337a8c9ec7e0c7)

## Affine/Softmax 层的实现
神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”{1[几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算。——译者注]}。因此，这里将进行仿射变换的处理实现为“Affine 层”。

### Affine 层

### 批版本的的 Affine 层

### Softmax-with-Loss 层
最后介绍一下输出层的 softmax 函数。前面我们提到过，softmax 函数会将输入值正规化之后再输出。比如手写数字识别时，Softmax 层的输出如图：
![](https://note.youdao.com/yws/api/personal/file/WEB286e1df6cedf604becb8a12da92ca472?method=download&shareKey=2a9b77c4b9d45dfe61c7096a1437f802)

输入图像通过 Affine 层和 ReLU 层进行转换，10 个输入通过 Softmax 层进行正规化。在这个例子中，“0”的得分是 5.3，这个值经过 Softmax 层转换为 0.008（0.8%）；“2”的得分是 10.1，被转换为 0.991（99.1%）

Softmax 层将输入值正规化（将输出值的和调整为 1）之后再输出。另外，因为手写数字识别要进行 10 类分类，所以向Softmax 层的输入也有 10 个。

神经网络中进行的处理有推理（inference）和学习两个阶段。神经网络的推理通常不使用 Softmax 层。比如，用图 5-28 的网络进行推理时，会将最后一个 Affine 层的输出作为识别结果。神经网络中未被正规化的输出结果（图 5-28 中 Softmax 层前面的 Affine 层的输出）有时被称为“得分”。也就是说，当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要 Softmax 层。不过，神经网络的学习阶段则需要 Softmax 层。

神经网络的反向传播会把这个差分表示的误差传递给前面的层，这是神经网络学习中的重要性质。

神经网络学习的目的就是通过调整权重参数，使神经网络的输出（Softmax 的输出）接近教师标签。因此，必须将神经网络的输出与教师标签的误差高效地传递给前面的层。刚刚的(y1-t1, y2-t2, y3-t3)正是 Softmax 层的输出与教师标签的差，直截了当地表示了当前神经网络的输出与教师标签的误差。

使用交叉熵误差作为 softmax 函数的损失函数后，反向传播得到(y1-t1, y2-t2, y3-t3)这样“漂亮”的结果。实际上，这样“漂亮”的结果并不是偶然的，而是为了得到这样的结果，特意设计了交叉熵误差函数。回归问题中输出层使用“恒等函数”，损失函数使用“平方和误差”，也是出于同样的理由（3.5 节）。也就是说，使用“平方和误差”作为“恒等函数”的损失函数，反向传播才能得到(y1-t1, y2-t2, y3-t3)这样“漂亮”的结果。

## 误差反向传播法的实现
### 神经网络学习的全貌图
```
前提
    神经网络中有合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为学习。神经网络的学习分为下面 4 个步骤。

步骤1： mini-batch
    从训练数据中随机选择一部分数据。
步骤2：计算梯度
    计算损失函数关于各个权重参数的梯度。
步骤3：更新参数
    将权重参数沿梯度方向进行微小的更新。
步骤4：重复
    重复步骤 1、步骤 2、步骤 3。
```

### 对应误差反向传播法的神经网络的实现

像这样通过将神经网络的组成元素以层的方式实现，可以轻松地构建神经网络。这个用层进行模块化的实现具有很大优点。因为想另外构建一个神经网络（比如 5 层、10 层、20 层……的大的神经网络）时，只需像组装乐高积木那样添加必要的层就可以了。之后，通过各个层内部实现的正向传播和反向传播，就可以正确计算进行识别处理或学习所需的梯度。

### 误差反向传播法的梯度确认

到目前为止，我们介绍了两种求梯度的方法。一种是基于数值微分的方法，另一种是解析性地求解数学式的方法。后一种方法通过使用误差反向传播法，即使存在大量的参数，也可以高效地计算梯度。因此，后文将不再使用耗费时间的数值微分，而是使用误差反向传播法求梯度。

数值微分的计算很耗费时间，而且如果有误差反向传播法的（正确的）实现的话，就没有必要使用数值微分的实现了。那么数值微分有什么用呢？实际上，在确认误差反向传播法的实现是否正确时，是需要用到数值微分的。

数值微分的优点是实现简单，因此，一般情况下不太容易出错。而误差反向传播法的实现很复杂，容易出错。所以，经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确。确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常相近）的操作称为梯度确认（gradient check）。


数值微分和误差反向传播法的计算结果之间的误差为 0 是很少见的。这是因为计算机的计算精度有限（比如，32 位浮点数）。受到数值精度的限制，刚才的误差一般不会为 0，但是如果实现正确的话，可以期待这个误差是一个接近 0 的很小的值。如果这个值很大，就说明误差反向传播法的实现存在错误。

### 使用误差反向传播法的学习


## 小结
本章我们介绍了将计算过程可视化的计算图，并使用计算图，介绍了神经网络中的误差反向传播法，并以层为单位实现了神经网络中的处理。我们学过的层有 ReLU 层、Softmax-with-Loss 层、Affine 层、Softmax 层等，这些层中实现了 forward 和 backward 方法，通过将数据正向和反向地传播，可以高效地计算权重参数的梯度。通过使用层进行模块化，神经网络中可以自由地组装层，轻松构建出自己喜欢的网络。

- 通过使用计算图，可以直观地把握计算过程。
- 计算图的节点是由局部计算构成的。局部计算构成全局计算。
- 计算图的正向传播进行一般的计算。通过计算图的反向传播，可以计算各个节点的导数。
- 通过将神经网络的组成元素实现为层，可以高效地计算梯度（反向传播法）。
- 通过比较数值微分和误差反向传播法的结果，可以确认误差反向传播法的实现是否正确（梯度确认）。

## Reference
1. [深度学习入门：基于Python的理论与实现](http://www.ituring.com.cn/book/1921)